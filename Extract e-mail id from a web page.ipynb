{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting e-mail id from the web page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We are going to learn about, how to extract an email ID on the given webpage.<br><br> \n",
    "    <h4>Step 1</h4>\n",
    "    We need to import all the essential libraries to our program.<br><br>\n",
    "    <code>BeautifulSoup</code> : It is a Python library for extracting data out of HTML and XML files.<br>\n",
    "    <code>requests</code> : The requests library allows us to send HTTP requests using Python.<br>\n",
    "    <code>urllib.parse</code> : This module provides functions for manipulating URLs and their component parts, to either break them down or build them up.<br> \n",
    "    <code>collections</code> : It provides different types of containers<br>\n",
    "    <code>re</code> : A module that handles regular expressions.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import requests.exceptions\n",
    "from urllib.parse import urlsplit\n",
    "from collections import deque\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "<h4>Step 2</h4>\n",
    "Select the url, for extracting email-ID from given url.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a queue of urls to be crawled\n",
    "new_urls = deque(['http://www.eshikshak.co.in/'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "<h4>Step 3</h4>\n",
    "We have to process the given url only once, so keep track of processed urls.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a set of urls that we have already crawled\n",
    "processed_urls = set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "<h4>Step 4</h4>\n",
    "While crawling the given url, we may encounter more than one the email-ID so keep them in the collections.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a set of crawled emails\n",
    "emails = set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "<h4>Step 5</h4>\n",
    "Time to start crawling, we need to crawl all the urls in the queue and maintain the list of crawled urls & get the page content from the webpage, if any error is encountered move to next page\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing http://www.eshikshak.co.in/\n"
     ]
    }
   ],
   "source": [
    "# process urls one by one until we exhaust the queue\n",
    "while len(new_urls):\n",
    "    # move next url from the queue to the set of processed urls\n",
    "    url = new_urls.popleft()\n",
    "    processed_urls.add(url)\n",
    "    # get url's content\n",
    "    print(\"Processing %s\" % url)\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "    except (requests.exceptions.MissingSchema, requests.exceptions.ConnectionError):\n",
    "        # ignore pages with errors \n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "<h4>Step 6</h4>\n",
    "Now we need to extract some base parts of the current url; essential part for transfering relative links found in the document into absolute ones:\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract base url and path to resolve relative links\n",
    "parts = urlsplit(url)\n",
    "base_url = \"{0.scheme}://{0.netloc}\".format(parts)\n",
    "path = url[:url.rfind('/')+1] if '/' in parts.path else url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "<h4>Step 7</h4>\n",
    "From the page content extract emailIDs and add them to emails set\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all email addresses and add them into the resulting set \n",
    "new_emails = set(re.findall(r\"[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-z]+\", response.text, re.I))\n",
    "emails.update(new_emails)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "<h4>Step 8</h4>\n",
    "Once the current page is processed, its time to search links to other pages and add them to url queue(that's the magic of crawling). Get a <code>Beautifulsoup</code> object for parsing html pages.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a beutiful soup for the html document\n",
    "soup = BeautifulSoup(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "<h4>Step 9</h4>\n",
    "<code>soup</code> object contains html elements, now find all the <code>anchor</code> tags with its <code>href</code> attributes to reslove relative links and keep a record of processed urls\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find and process all the anchors in the document\n",
    "for anchor in soup.find_all(\"a\"):\n",
    "    # extract link url from the anchor\n",
    "    link = anchor.attrs[\"href\"] if \"href\" in anchor.attrs else ''\n",
    "    # resolve relative links\n",
    "    if link.startswith('/'):\n",
    "        link = base_url + link\n",
    "    elif not link.startswith('http'):\n",
    "        link = path + link\n",
    "    # add the new url to the queue if it was not enqueued nor processed yet\n",
    "    if not link in new_urls and not link in processed_urls:\n",
    "        new_urls.append(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "<h4>Step 10</h4>\n",
    "List out all the email-ID extracted from the given url\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prakashgkhaire@gmail.com\n",
      "eshikshak.co.in@gmail.com\n"
     ]
    }
   ],
   "source": [
    "for email in emails:\n",
    "    print(email)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "<h2>About Author</h2>\n",
    "Name : <b>Prakash G. Khaire</b><br>\n",
    "    Bio  : <b>Software Consultant </b> :::: <b>Online Tutor </b> :::: <b>HOD & Assitant Professor</b><br>\n",
    "           Contact Details :\n",
    "    <mobile><b>+91 951 0446 143</b></mobile><br> \n",
    "    <a href=\"http://www.eshikshak.co.in\">www.eshikshak.co.in</a>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
